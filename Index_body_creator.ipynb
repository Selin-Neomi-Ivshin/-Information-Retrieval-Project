{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"ZJUHIkQ7A9In"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import math\n","import pyspark\n","import sys\n","from collections import Counter, OrderedDict, defaultdict\n","import itertools\n","from itertools import islice, count, groupby\n","import pandas as pd\n","import os\n","import re\n","from operator import itemgetter\n","import nltk\n","from nltk.stem.porter import *\n","from nltk.corpus import stopwords\n","from time import time\n","from pathlib import Path\n","import pickle\n","import pandas as pd\n","from google.cloud import storage\n","from contextlib import closing\n","from pyspark.sql import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf, SparkFiles\n","from pyspark.sql import SQLContext\n","import hashlib\n","\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n","\u001B[0m"]}],"source":["!pip install -q google-cloud-storage==1.43.0"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"ls5Dde3yhjOR"},"outputs":[],"source":["# Put your bucket name below and make sure you can access it without an error\n","bucket_name_body = 'project_ir313254724_322769175_body'\n","full_path = f\"gs://{bucket_name_body}/\"\n","paths=[]\n","\n","client = storage.Client()\n","blobs = client.list_blobs(bucket_name_body)\n","for b in blobs:\n","    if b.name != 'graphframes.sh':\n","        paths.append(full_path+b.name)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"-SSKe2tKBTU0"},"outputs":[],"source":["RE_TOKENIZE = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n","\n","def tokenize(text):\n","    return [token.group() for token in RE_TOKENIZE.finditer(text.lower()) if (token.group().strip() and token.group() not in stop_words)]\n","\n","def word_count(text, id, stem=False):\n","    ''' Count the frequency of each word in text (tf) that is not included in\n","    all_stopwords and return entries that will go into our posting lists.\n","    Parameters:\n","    -----------\n","    text: str\n","    Text of one document\n","    id: int\n","    Document id\n","    Returns:\n","    --------\n","    List of tuples\n","    A list of (token, (doc_id, tf)) pairs\n","    for example: [(\"Anarchism\", (12, 5)), ...]\n","    '''\n","    global_dict = {}\n","    all_tokens = tokenize(text)\n","    tokens = stemming(all_tokens, stem)\n","    for token in tokens:\n","      val = global_dict.get(token)\n","      if val is None:\n","        global_dict[token] = 1\n","      else:\n","        global_dict[token] += 1\n","    final = [(token, (id, freq)) for token, freq in global_dict.items()]\n","    return final\n","\n","'''\n","sorting the posting list by doc_id. we will apply this function with map reduce\n","and it will be ended as dict of token: posting_list key-value.\n","'''\n","def reduce_word_counts(unsorted_pl):\n","    ''' Returns a sorted posting list by wiki_id.\n","    Parameters:\n","    -----------\n","    unsorted_pl: list of tuples\n","    A list of (wiki_id, tf) tuples\n","    Returns:\n","    --------\n","    list of tuples\n","    A sorted posting list.\n","    '''\n","    sorted_pl = sorted(unsorted_pl, key=lambda x: x[0])  # Sorting by wiki_id\n","    return sorted_pl\n","\n","'''\n","idf added to this dict. this will be dict of {token: [df, idf]} and it will help to calculate the BM25.\n","'''\n","def calculate_df(postings, N):\n","    ''' Takes a posting list RDD and calculate the df for each token.\n","    Parameters:\n","    -----------\n","    postings: RDD\n","    An RDD where each element is a (token, posting_list) pair.\n","    Returns:\n","    --------\n","    RDD\n","    An RDD where each element is a (token, df) pair.\n","    '''\n","    return postings.map(lambda x: (x[0], (len(x[1]), math.log(N / len(x[1])))))\n","\n","def _hash(s):\n","    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n","\n","def token2bucket_id(token):\n","    NUM_BUCKETS = 124\n","    return int(_hash(token),16) % NUM_BUCKETS\n","\n","def partition_postings_and_write(postings):\n","    ''' A function that partitions the posting lists into buckets, writes out\n","    all posting lists in a bucket to disk, and returns the posting locations for\n","    each bucket. Partitioning should be done through the use of token2bucket\n","    above. Writing to disk should use the function  write_a_posting_list, a\n","    static method implemented in inverted_index_colab.py under the InvertedIndex\n","    class.\n","    Parameters:\n","    -----------\n","    postings: RDD\n","    An RDD where each item is a (w, posting_list) pair.\n","    Returns:\n","    --------\n","    RDD\n","    An RDD where each item is a posting locations dictionary for a bucket. The\n","    posting locations maintain a list for each word of file locations and\n","    offsets its posting list was written to. See write_a_posting_list for\n","    more details.\n","    '''\n","    #create rdd of (bucket_id,(w, posting_list))\n","    bucket_postrdd = postings.map(lambda post_rdd: (token2bucket_id(post_rdd[0]),post_rdd)).groupByKey()\n","    final = bucket_postrdd.map(lambda buck: InvertedIndex.write_a_posting_list(buck,\"body_posting/\",bucket_name_body))\n","    return final\n","\n","\n","def stemming(tokens, stem=False):\n","    length_list = len(tokens)\n","    if stem:\n","      stemmer = PorterStemmer()\n","      index = 0\n","      while index < length_list:\n","          tokens[index] = stemmer.stem(tokens[index])\n","          index += 1\n","    return tokens"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"60BYav4iBZTD"},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["parquetFile = spark.read.parquet(*paths)\n","doc_text_pairs_text = parquetFile.select(\"text\", \"id\").rdd"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"iw8YJrOrBduR"},"outputs":[{"name":"stdout","output_type":"stream","text":["inverted_index_gcp.py\r\n"]}],"source":["%cd -q /home/dataproc\n","!ls inverted_index_gcp.py"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"oQJmGEHYBgZn"},"outputs":[],"source":["# adding our python module to the cluster\n","sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n","sys.path.insert(0,SparkFiles.getRootDirectory())\n","from inverted_index_gcp import *"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"kLnG5YUFBjMZ"},"outputs":[],"source":["stop_words1 = set(stopwords.words('english'))\n","corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\",\n","                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\",\n","                    \"part\", \"thumb\", \"including\", \"second\", \"following\",\n","                    \"many\", \"however\", \"would\", \"became\"]\n","\n","stop_words = stop_words1.union(corpus_stopwords)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"hHGIagalBnHB"},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# Tokenize each document and count the number of tokens\n","doc_token_counts_body = doc_text_pairs_text.map(lambda x: (x[1], len(tokenize(x[0]))))\n","\n","# Calculate the total number of documents and the total number of tokens\n","total_documents_body = doc_text_pairs_text.count()\n","total_tokens_body = doc_token_counts_body.map(lambda x: x[1]).sum()\n","\n","# Calculate the average number of tokens per document\n","avg_tokens_per_doc_body = total_tokens_body / total_documents_body"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# word counts map\n","flag_body = True\n","word_counts_body = doc_text_pairs_text.flatMap(lambda x: word_count(x[0], x[1], flag_body))\n","postings_body = word_counts_body.groupByKey().mapValues(lambda x: reduce_word_counts(list(x)))\n","postings_filtered = postings_body.filter(lambda x: len(x[1]) > 50)\n","w2df_body = calculate_df(postings_filtered, total_documents_body)\n","w2df_dict_body = w2df_body.collectAsMap()\n","\n","# partition posting lists and write out\n","_body = partition_postings_and_write(postings_filtered).collect()"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"Vwlt1qQHBscP"},"outputs":[],"source":["# collect all posting lists locations into one super-set\n","super_posting_locs_body = defaultdict(list)\n","for blob in client.list_blobs(bucket_name_body, prefix='body_posting'):\n","  if not blob.name.endswith(\"pickle\"):\n","    continue\n","  with blob.open(\"rb\") as f:\n","    posting_locs = pickle.load(f)\n","    for k, v in posting_locs.items():\n","      super_posting_locs_body[k].extend(v)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"QOWXw1PaBv1E"},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# Create inverted index instance\n","inverted_body = InvertedIndex()\n","# Adding the posting locations dictionary to the inverted index\n","inverted_body.posting_locs = super_posting_locs_body\n","# Add the token - df dictionary to the inverted index\n","inverted_body.df = w2df_dict_body\n","inverted_body.stem = flag_body\n","\n","'''\n","insert important values into InvertedIndex object.\n","'''\n","inverted_body.N = total_documents_body\n","inverted_body.avg_doclen = avg_tokens_per_doc_body\n","inverted_body.docs_len = doc_token_counts_body.collectAsMap() # Dict of doc_id: len_doc."]},{"cell_type":"code","execution_count":13,"metadata":{"id":"H_eUrDFXB0UO"},"outputs":[{"name":"stdout","output_type":"stream","text":["Copying file://body_index.pkl [Content-Type=application/octet-stream]...\n","/ [1 files][ 65.4 MiB/ 65.4 MiB]                                                \n","Operation completed over 1 objects/65.4 MiB.                                     \n"]}],"source":["inverted_body.write_index('.', 'body_index')\n","index_src_body = \"body_index.pkl\"\n","index_dst_body = f'gs://{bucket_name_body}/body_posting/{index_src_body}'\n","!gsutil cp $index_src_body $index_dst_body\n","inverted_body.write_index('.', 'body_index')"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":1}