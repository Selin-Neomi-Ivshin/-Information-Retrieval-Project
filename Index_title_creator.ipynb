{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"98MjvyW5_yr8"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import math\n","import pyspark\n","import sys\n","from collections import Counter, OrderedDict, defaultdict\n","import itertools\n","from itertools import islice, count, groupby\n","import pandas as pd\n","import os\n","import re\n","from operator import itemgetter\n","import nltk\n","from nltk.stem.porter import *\n","from nltk.corpus import stopwords\n","from time import time\n","from pathlib import Path\n","import pickle\n","import pandas as pd\n","from google.cloud import storage\n","from contextlib import closing\n","from pyspark.sql import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf, SparkFiles\n","from pyspark.sql import SQLContext\n","\n","\n","import hashlib\n","\n","\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n","\u001B[0m"]}],"source":["!pip install -q google-cloud-storage==1.43.0"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"Ch29_nvbALkR"},"outputs":[],"source":["# Put your bucket name below and make sure you can access it without an error\n","bucket_name = 'project_ir313254724_322769175_title_final'\n","full_path = f\"gs://{bucket_name}/\"\n","paths=[]\n","\n","client = storage.Client()\n","blobs = client.list_blobs(bucket_name)\n","for b in blobs:\n","    if b.name != 'graphframes.sh':\n","        paths.append(full_path+b.name)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"J45gdD3aAXZw"},"outputs":[],"source":["def tokenize(text):\n","    def get_date_pattern():\n","      return r'((Jan(uary)?|Mar(ch)?|May|Jul(y)?|Aug(ust)?|Oct(ober)?|Dec(ember)?)\\s([1-2][0-9]|[1-9]|[3][01]),\\s([1-2][0-9]{3})|(Feb(ruary)?)\\s([1-2][0-9]|[1-9]),\\s([1-2][0-9]{3})|(Apr(il)?|Jun(e)?|Sep(tember)?|Nov(ember)?)\\s([1-2][0-9]|[1-9]|30),\\s([1-2][0-9]{3})|(([1-2][0-9]|[1-9]|3[01])\\s(Jan(uary)?|Mar(ch)?|May|Jul(y)?|Aug(ust)?|Oct(ober)?|Dec(ember)?)\\s[1-2][0-9]{3})|(([1-2][0-9]|[1-9])\\s(Feb(uary)?)\\s[1-2][0-9]{3})|(([1-2][0-9]|[1-9]|30)\\s(Apr(il)?|Jun(e)?|Sep(tember)?|Nov(ember)?)\\s[1-2][0-9]{3}))'\n","    def get_time_pattern():\n","      return r'(\\b([1-9]|(1[0-2]))\\.([0-5][0-9])(AM|PM)|(0[0-9]|1[0-2])[0-5][0-9](a\\.m\\.|p\\.m\\.)|\\b([1-9]|1[0-9]|2[0-3]|00):[0-5][0-9]:[0-5][0-9]\\b)'\n","    def get_percent_pattern():\n","      return r'((([1-9][0-9]{0,2})+(\\,[0-9]{3})*|0)(\\.\\d+)?)%'\n","    def get_number_pattern():\n","      return r'(?<![\\w\\+\\-\\,\\.])([+-]?[1-9]\\d{0,2}((\\,\\d{3})*|\\d*)|0)(\\.\\d+)?(?!(\\.[^\\s]|\\,[^\\s]|\\w))'\n","    def get_word_pattern():\n","      return r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\"\n","\n","    RE_TOKENIZE = re.compile(rf\"\"\"\n","    (\n","        # dates\n","        |(?P<DATE>{get_date_pattern()})\n","        # time\n","        |(?P<TIME>{get_time_pattern()})\n","        # Percents\n","        |(?P<PERCENT>{get_percent_pattern()})\n","        # Numbers\n","        |(?P<NUMBER>{get_number_pattern()})\n","        # Words\n","        |(?P<WORD>{get_word_pattern()})\n","        )\"\"\",  re.MULTILINE | re.IGNORECASE | re.VERBOSE | re.UNICODE)\n","    return [token.group() for token in RE_TOKENIZE.finditer(text.lower()) if (token.group().strip() and token.group() not in stop_words)]\n","\n","def word_count(text, id, stem=False):\n","    ''' Count the frequency of each word in text (tf) that is not included in\n","    all_stopwords and return entries that will go into our posting lists.\n","    Parameters:\n","    -----------\n","    text: str\n","    Text of one document\n","    id: int\n","    Document id\n","    Returns:\n","    --------\n","    List of tuples\n","    A list of (token, (doc_id, tf)) pairs\n","    for example: [(\"Anarchism\", (12, 5)), ...]\n","    '''\n","    global_dict = {}\n","    all_tokens = tokenize(text)\n","    tokens = stemming(all_tokens, stem)\n","    for token in tokens:\n","      val = global_dict.get(token)\n","      if val is None:\n","        global_dict[token] = 1\n","      else:\n","        global_dict[token] += 1\n","    final = [(token, (id, freq)) for token, freq in global_dict.items()]\n","    return final\n","\n","'''\n","sorting the posting list by doc_id. we will apply this function with map reduce\n","and it will be ended as dict of token: posting_list key-value.\n","'''\n","def reduce_word_counts(unsorted_pl):\n","    ''' Returns a sorted posting list by wiki_id.\n","    Parameters:\n","    -----------\n","    unsorted_pl: list of tuples\n","    A list of (wiki_id, tf) tuples\n","    Returns:\n","    --------\n","    list of tuples\n","    A sorted posting list.\n","    '''\n","    sorted_pl = sorted(unsorted_pl, key=lambda x: x[0])  # Sorting by wiki_id\n","    return sorted_pl\n","\n","'''\n","idf added to this dict. this will be dict of {token: [df, idf]} and it will help to calculate the BM25.\n","'''\n","def calculate_df(postings, N):\n","    ''' Takes a posting list RDD and calculate the df for each token.\n","    Parameters:\n","    -----------\n","    postings: RDD\n","    An RDD where each element is a (token, posting_list) pair.\n","    Returns:\n","    --------\n","    RDD\n","    An RDD where each element is a (token, df) pair.\n","    '''\n","    return postings.map(lambda x: (x[0], (len(x[1]), math.log(N / len(x[1])))))\n","\n","def _hash(s):\n","    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n","\n","def token2bucket_id(token):\n","    NUM_BUCKETS = 124\n","    return int(_hash(token),16) % NUM_BUCKETS\n","\n","def partition_postings_and_write(postings):\n","    ''' A function that partitions the posting lists into buckets, writes out\n","    all posting lists in a bucket to disk, and returns the posting locations for\n","    each bucket. Partitioning should be done through the use of token2bucket\n","    above. Writing to disk should use the function  write_a_posting_list, a\n","    static method implemented in inverted_index_colab.py under the InvertedIndex\n","    class.\n","    Parameters:\n","    -----------\n","    postings: RDD\n","    An RDD where each item is a (w, posting_list) pair.\n","    Returns:\n","    --------\n","    RDD\n","    An RDD where each item is a posting locations dictionary for a bucket. The\n","    posting locations maintain a list for each word of file locations and\n","    offsets its posting list was written to. See write_a_posting_list for\n","    more details.\n","    '''\n","    #create rdd of (bucket_id,(w, posting_list))\n","    bucket_postrdd = postings.map(lambda post_rdd: (token2bucket_id(post_rdd[0]),post_rdd)).groupByKey()\n","    final = bucket_postrdd.map(lambda buck: InvertedIndex.write_a_posting_list(buck,\"title_posting/\",bucket_name))\n","    return final\n","\n","def stemming(tokens, stem=False):\n","    length_list = len(tokens)\n","    if stem:\n","      stemmer = PorterStemmer()\n","      index = 0\n","      while index < length_list:\n","          tokens[index] = stemmer.stem(tokens[index])\n","          index += 1\n","    return tokens"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"UA_mwfd3AdDq"},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["parquetFile = spark.read.parquet(*paths)\n","doc_text_pairs_title = parquetFile.select(\"title\", \"id\").rdd"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"1sSZZuIuieAa"},"outputs":[{"name":"stdout","output_type":"stream","text":["inverted_index_gcp.py\r\n"]}],"source":["%cd -q /home/dataproc\n","!ls inverted_index_gcp.py"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"ZBkReseaAg5k"},"outputs":[],"source":["sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n","sys.path.insert(0,SparkFiles.getRootDirectory())\n","from inverted_index_gcp import *"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"1fIsKMbyApVj"},"outputs":[],"source":["stop_words1 = set(stopwords.words('english'))\n","corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n","                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n","                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n","                    \"many\", \"however\", \"would\", \"became\"]\n","\n","stop_words = stop_words1.union(corpus_stopwords)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"34qqekdcAp2p"},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# Tokenize each document and count the number of tokens\n","doc_token_counts_title = doc_text_pairs_title.map(lambda x: (x[1], len(tokenize(x[0]))))\n","\n","# Calculate the total number of documents and the total number of tokens\n","total_documents_title = doc_text_pairs_title.count()\n","total_tokens_title = doc_token_counts_title.map(lambda x: x[1]).sum()\n","\n","# Calculate the average number of tokens per document\n","avg_tokens_per_doc_title = total_tokens_title / total_documents_title\n","\n","# word counts map\n","flag_title = True\n","# model = Word2Vec(vector_size=300, window=10, min_count=5, workers=4)\n","word_counts_title = doc_text_pairs_title.flatMap(lambda x: word_count(x[0], x[1], flag_title))\n","\n","postings_title = word_counts_title.groupByKey().mapValues(lambda x: reduce_word_counts(list(x)))\n","w2df_title = calculate_df(postings_title, total_documents_title)\n","w2df_dict_title = w2df_title.collectAsMap()\n","\n","# partition posting lists and write out\n","_ = partition_postings_and_write(postings_title).collect()"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"aEIkDCA9Asz6"},"outputs":[],"source":["# collect all posting lists locations into one super-set\n","super_posting_locs_title = defaultdict(list)\n","for blob in client.list_blobs(bucket_name, prefix='title_posting'):\n","  if not blob.name.endswith(\"pickle\"):\n","    continue\n","  with blob.open(\"rb\") as f:\n","    posting_locs = pickle.load(f)\n","    for k, v in posting_locs.items():\n","      super_posting_locs_title[k].extend(v)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting gensim\n","  Downloading gensim-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n","Requirement already satisfied: numpy>=1.18.5 in /opt/conda/miniconda3/lib/python3.10/site-packages (from gensim) (1.22.4)\n","Requirement already satisfied: scipy>=1.7.0 in /opt/conda/miniconda3/lib/python3.10/site-packages (from gensim) (1.9.3)\n","Collecting smart-open>=1.8.1 (from gensim)\n","  Downloading smart_open-7.0.1-py3-none-any.whl.metadata (23 kB)\n","Requirement already satisfied: wrapt in /opt/conda/miniconda3/lib/python3.10/site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n","Downloading gensim-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m26.5/26.5 MB\u001B[0m \u001B[31m71.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\n","\u001B[?25hDownloading smart_open-7.0.1-py3-none-any.whl (60 kB)\n","\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m60.8/60.8 kB\u001B[0m \u001B[31m8.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n","\u001B[?25hInstalling collected packages: smart-open, gensim\n","Successfully installed gensim-4.3.2 smart-open-7.0.1\n","\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m"]}],"source":["!pip install gensim"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[===-----------------------------------------------] 6.2% 23.4/376.1MB downloaded"]},{"name":"stderr","output_type":"stream","text":["IOPub message rate exceeded.\n","The notebook server will temporarily stop sending output\n","to the client in order to avoid crashing it.\n","To change this limit, set the config variable\n","`--NotebookApp.iopub_msg_rate_limit`.\n","\n","Current values:\n","NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n","NotebookApp.rate_limit_window=3.0 (secs)\n","\n"]},{"name":"stdout","output_type":"stream","text":["[=======-------------------------------------------] 15.9% 59.9/376.1MB downloaded"]},{"name":"stderr","output_type":"stream","text":["IOPub message rate exceeded.\n","The notebook server will temporarily stop sending output\n","to the client in order to avoid crashing it.\n","To change this limit, set the config variable\n","`--NotebookApp.iopub_msg_rate_limit`.\n","\n","Current values:\n","NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n","NotebookApp.rate_limit_window=3.0 (secs)\n","\n"]},{"name":"stdout","output_type":"stream","text":["[==========----------------------------------------] 21.0% 78.9/376.1MB downloaded"]}],"source":["import gensim.models\n","import gensim.downloader as api\n","model_glove = api.load(\"glove-wiki-gigaword-300\")"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"A0nGeEAJAwIB"},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# Create inverted index instance\n","inverted_title = InvertedIndex()\n","# Adding the posting locations dictionary to the inverted index\n","inverted_title.posting_locs = super_posting_locs_title\n","# Add the token - df dictionary to the inverted index\n","inverted_title.df = w2df_dict_title\n","inverted_title.stem = flag_title\n","inverted_title.stop_words = stop_words\n","inverted_title.model = model_glove\n","\n","'''\n","insert important values into InvertedIndex object.\n","'''\n","inverted_title.N = total_documents_title\n","inverted_title.avg_doclen = avg_tokens_per_doc_title\n","inverted_title.docs_len = doc_token_counts_title.collectAsMap() # Dict of doc_id: len_doc."]},{"cell_type":"code","execution_count":14,"metadata":{"id":"urZgWZmfAzCc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Copying file://title_index.pkl [Content-Type=application/octet-stream]...\n","==> NOTE: You are uploading one or more large file(s), which would run          \n","significantly faster if you enable parallel composite uploads. This\n","feature can be enabled by editing the\n","\"parallel_composite_upload_threshold\" value in your .boto\n","configuration file. However, note that if you do this large files will\n","be uploaded as `composite objects\n","<https://cloud.google.com/storage/docs/composite-objects>`_,which\n","means that any user who downloads such objects will need to have a\n","compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n","without a compiled crcmod, computing checksums on composite objects is\n","so slow that gsutil disables downloads of composite objects.\n","\n","\\ [1 files][590.8 MiB/590.8 MiB]   29.5 MiB/s                                   \n","Operation completed over 1 objects/590.8 MiB.                                    \n"]}],"source":["inverted_title.write_index('.', 'title_index')\n","index_src_title = \"title_index.pkl\"\n","index_dst_title = f'gs://{bucket_name}/title_posting/{index_src_title}'\n","!gsutil cp $index_src_title $index_dst_title\n","inverted_title.write_index('.', 'title_index')"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":1}