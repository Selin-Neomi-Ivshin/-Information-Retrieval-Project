# -*- coding: utf-8 -*-
"""TestForQueries (1) (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SEku1XROOa9KVf48I3GxZhgVSvoiZNCc
"""

# from google.cloud import storage

# Commented out IPython magic to ensure Python compatibility.
# %cd -q /home/dataproc
# !ls inverted_index_gcp.py
from inverted_index_gcp import *

# !pip install gensim
# from gensim import downloader as api

import math
import sys
from collections import Counter, OrderedDict, defaultdict
import itertools
from itertools import islice, count, groupby
import pandas as pd
import os
import re
from operator import itemgetter
import nltk
from nltk.stem.porter import *
from nltk.corpus import stopwords
from time import time
from pathlib import Path
import pickle
import pandas as pd
from contextlib import closing
from concurrent.futures import ThreadPoolExecutor
import threading


import hashlib
def _hash(s):
    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()

import math
import threading
from multiprocessing import Pool
import heapq



class QueryProcessor:
    def __init__(self, bucket_index1, bucket_index2, max_pr , max_pv, num=100):
      self.bucket_name_title = bucket_index1
      self.bucket_name_body = bucket_index2
      self.bm25_scores = {}
      self.doc_locks = {}
      self.lock = threading.Lock()
      self.tokens_stem_title = None
      self.tokens_stem_body = None
      self.tokens_tf_title = None
      self.tokens_tf_body = None
      self.num = num
      self.max_bm = 0
      self.max_pr = max_pr
      self.max_pv = max_pv

    def process_query(self, query, pagerank, pageview, docid_title, index1, index2):
      # Tokenization and stemming
      stem1, stem2 = index1.stem, index2.stem
      stop_words = index1.stop_words
      model = index1.model
      self.word_count(query, (stem1, stem2), stop_words)

      # Your existing process_query method
      self.find_similar_words(0.7, model)

      # Parallel processing of terms
      self.parallel_process_terms(index1, index2)

      # Rank documents based on BM25 scores
      return self.rank_documents(pagerank, pageview, docid_title)

    def parallel_process_terms(self, index1, index2):
      tokens_tf_title_list = list(self.tokens_tf_title.items())
      tokens_tf_body_list = list(self.tokens_tf_body.items())
      for i in range(max(len(tokens_tf_title_list), len(tokens_tf_body_list))):
        if i < len(tokens_tf_title_list) and i < len(tokens_tf_body_list):
          term = (tokens_tf_title_list[i][0], tokens_tf_body_list[i][0])
          tf = (tokens_tf_title_list[i][1], tokens_tf_body_list[i][1])
        elif i < len(tokens_tf_title_list):
          term = (tokens_tf_title_list[i][0], None)
          tf = (tokens_tf_title_list[i][1], None)
        elif i < len(tokens_tf_body_list):
          term = (None, tokens_tf_body_list[i][0])
          tf = (None, tokens_tf_body_list[i][1])
        self.process_term(term, tf, index1, index2)

    def process_term(self, term, tf, index1, index2):
      term_title, term_body = term
      tf_title, tf_body = tf

      if term_title:
        val_title = index1.df.get(term_title)
        if val_title:
          posting_list = self.read_posting_list('.', term_title, self.bucket_name_title, index1)
          self.calculate_bm25((term_title, tf_title), posting_list, 'title', index1)

      if term_body:
        val_body = index2.df.get(term_body)
        if val_body:
          posting_list = self.read_posting_list('.', term_body, self.bucket_name_body, index2)
          self.calculate_bm25((term_body, tf_body), posting_list, 'body', index2)

    def jumper(self, pl, indx, index, tf_query):
        if indx == 'title':
            weight = 0.4
        elif indx == 'body':
            weight = 0.6

        term, tf = tf_query
        df, idf = index.df.get(term, (0, 0))

        N = index.N
        avg_doc_length = index.avg_doclen

        k1 = 1.5
        k3 = 1.5
        b = 0.5

        for doc_id, tf_doc in pl:
            doc_length = index.docs_len.get(doc_id, 0)
            B = (1-b+b*(doc_length/avg_doc_length))
            bm25 = idf * (((k1 + 1)*tf_doc)/(B*k1+tf_doc)) * (((k3+1)*tf)/(k3+tf))
            bm25 = bm25 * weight

            if doc_id not in self.bm25_scores:
              self.bm25_scores[doc_id] = 0
            self.bm25_scores[doc_id] += bm25
            if self.bm25_scores.get(doc_id) > self.max_bm:
              self.max_bm = self.bm25_scores.get(doc_id)



    def calculate_bm25(self, tf_query, pl, indx, index):
      jumps = len(pl) // 10
      pl_list = []
      for i in range(10):
        if i == 9:
            pl_list.append(pl[i*jumps:])
        else:
            pl_list.append(pl[i*jumps:(i+1)*jumps])
      threads = []
      for i in range(10):
        thread = threading.Thread(target=self.jumper, args=(pl_list[i], indx, index, tf_query))
        threads.append(thread)

      for thread in threads:
        thread.start()
      for thread in threads:
        thread.join()

    @staticmethod
    def tokenize_title(text, stop_words):
      def get_date_pattern():
        return r'((Jan(uary)?|Mar(ch)?|May|Jul(y)?|Aug(ust)?|Oct(ober)?|Dec(ember)?)\s([1-2][0-9]|[1-9]|[3][01]),\s([1-2][0-9]{3})|(Feb(ruary)?)\s([1-2][0-9]|[1-9]),\s([1-2][0-9]{3})|(Apr(il)?|Jun(e)?|Sep(tember)?|Nov(ember)?)\s([1-2][0-9]|[1-9]|30),\s([1-2][0-9]{3})|(([1-2][0-9]|[1-9]|3[01])\s(Jan(uary)?|Mar(ch)?|May|Jul(y)?|Aug(ust)?|Oct(ober)?|Dec(ember)?)\s[1-2][0-9]{3})|(([1-2][0-9]|[1-9])\s(Feb(uary)?)\s[1-2][0-9]{3})|(([1-2][0-9]|[1-9]|30)\s(Apr(il)?|Jun(e)?|Sep(tember)?|Nov(ember)?)\s[1-2][0-9]{3}))'
      def get_time_pattern():
        return r'(\b([1-9]|(1[0-2]))\.([0-5][0-9])(AM|PM)|(0[0-9]|1[0-2])[0-5][0-9](a\.m\.|p\.m\.)|\b([1-9]|1[0-9]|2[0-3]|00):[0-5][0-9]:[0-5][0-9]\b)'
      def get_percent_pattern():
        return r'((([1-9][0-9]{0,2})+(\,[0-9]{3})*|0)(\.\d+)?)%'
      def get_number_pattern():
        return r'(?<![\w\+\-\,\.])([+-]?[1-9]\d{0,2}((\,\d{3})*|\d*)|0)(\.\d+)?(?!(\.[^\s]|\,[^\s]|\w))'
      def get_word_pattern():
        return r"""[\#\@\w](['\-]?\w){2,24}"""

      RE_TOKENIZE = re.compile(rf"""
      (
          # dates
          |(?P<DATE>{get_date_pattern()})
          # time
          |(?P<TIME>{get_time_pattern()})
          # Percents
          |(?P<PERCENT>{get_percent_pattern()})
          # Numbers
          |(?P<NUMBER>{get_number_pattern()})
          # Words
          |(?P<WORD>{get_word_pattern()})
          )""",  re.MULTILINE | re.IGNORECASE | re.VERBOSE | re.UNICODE)
      return [token.group() for token in RE_TOKENIZE.finditer(text.lower()) if (token.group().strip() and token.group() not in stop_words)]

    @staticmethod
    def tokenize(text, stop_words):
      RE_TOKENIZE = re.compile(r"""[\#\@\w](['\-]?\w){2,24}""", re.UNICODE)
      return [token.group() for token in RE_TOKENIZE.finditer(text.lower()) if (token.group().strip() and token.group() not in stop_words)]

    def stemming(self, tokens, stem_indexs = (False, False)):
        stem_title = stem_indexs[0]
        stem_body = stem_indexs[1]
        stemmer = PorterStemmer()
        tokens_title = tokens[0]
        tokens_body = tokens[1]
        length_list_title = len(tokens_title)
        length_list_body = len(tokens_body)

        if stem_title:
          for i in range(length_list_title):
            tokens_title[i] = stemmer.stem(tokens_title[i])

        if stem_body:
          for i in range(length_list_body):
            tokens_body[i] = stemmer.stem(tokens_body[i])

        self.tokens_stem_title = tokens_title
        self.tokens_stem_body = tokens_body

    def word_count(self, text, stem_indexs = (False, False), stop_words = []):
      ''' Count the frequency of each word in text (tf) that is not included in
      all_stopwords and return entries that will go into our posting lists.
      Parameters:
      -----------
      text: str
      Text of one document
      id: int
      Document id
      Returns:
      --------
      List of tuples
      A list of (token, (doc_id, tf)) pairs
      for example: [("Anarchism", (12, 5)), ...]
      '''
      global_dict_title = {}
      global_dict_body = {}
      all_tokens_title = QueryProcessor.tokenize_title(text, stop_words)
      if len(all_tokens_title) <= 2:
          self.stemming((all_tokens_title, []), stem_indexs)
          l_title = len(self.tokens_stem_title)
          for i in range(l_title):
              val_title = global_dict_title.get(self.tokens_stem_title[i])
              if val_title is None:
                global_dict_title[self.tokens_stem_title[i]] = 0
              global_dict_title[self.tokens_stem_title[i]] += 1
      else:
          all_tokens_body = QueryProcessor.tokenize(text, stop_words)
          self.stemming((all_tokens_title, all_tokens_body), stem_indexs)
          l_title, l_body = len(self.tokens_stem_title), len(self.tokens_stem_body)
          for i in range(max(l_title, l_body)):
            if i < l_title and i < l_body:
              val_title = global_dict_title.get(self.tokens_stem_title[i])
              val_body = global_dict_body.get(self.tokens_stem_body[i])
              if val_title is None:
                global_dict_title[self.tokens_stem_title[i]] = 0
              global_dict_title[self.tokens_stem_title[i]] += 1
              if val_body is None:
                global_dict_body[self.tokens_stem_body[i]] = 0
              global_dict_body[self.tokens_stem_body[i]] += 1
            elif i < l_title:
              val_title = global_dict_title.get(self.tokens_stem_title[i])
              if val_title is None:
                global_dict_title[self.tokens_stem_title[i]] = 0
              global_dict_title[self.tokens_stem_title[i]] += 1
            elif i < l_body:
              val_body = global_dict_body.get(self.tokens_stem_body[i])
              if val_body is None:
                global_dict_body[self.tokens_stem_body[i]] = 0
              global_dict_body[self.tokens_stem_body[i]] += 1
      self.tokens_tf_title = global_dict_title
      self.tokens_tf_body = global_dict_body

    def find_similar_words(self, threshold, model):
      len_query_body = len(self.tokens_tf_body)
      len_query_title = len(self.tokens_tf_title)

      # For the body (long)
      if len_query_body > 4:
        similar_with_thresh = []
        query_tokens_list = list(self.tokens_tf_body.keys())
        try:
            similar_words = model.most_similar(positive = query_tokens_list, topn = 2)
            similar_with_thresh += [(similarity, word) for word, similarity in similar_words if similarity > threshold]
            for similarity, token in similar_with_thresh:
              if self.tokens_tf_body.get(token) is None:
                self.tokens_tf_body[token] = 0
              self.tokens_tf_body[token] += similarity
        except:
            pass
      # For the body (short)
      elif len_query_body <= 4:
        threads = []
        key_body = list(self.tokens_tf_body.keys())
        for i in range(len(self.tokens_tf_body.items())):
            token_body = key_body[i]
            thread = threading.Thread(target=self.find_similar_words_thread, args=(token_body, threshold, model, 'body'))
            threads.append(thread)

        for thread in threads:
            thread.start()
        for thread in threads:
            thread.join()

      # For the title (long)
      if len_query_title > 4:
        similar_with_thresh = []
        query_tokens_list = list(self.tokens_tf_title.keys())
        try:
            similar_words = model.most_similar(positive = query_tokens_list, topn = 2)
            similar_with_thresh += [(similarity, word) for word, similarity in similar_words if similarity > threshold]
            for similarity, token in similar_with_thresh:
              if self.tokens_tf_title.get(token) is None:
                self.tokens_tf_title[token] = 0
              self.tokens_tf_title[token] += similarity
        except:
            pass
      # For the title (short)
      elif len_query_title <= 4:
        threads = []
        key_title = list(self.tokens_tf_title.keys())
        for i in range(len(self.tokens_tf_title.items())):
            token_title = key_title[i]
            thread = threading.Thread(target=self.find_similar_words_thread, args=(token_title, threshold, model, 'title'))
            threads.append(thread)

        for thread in threads:
            thread.start()
        for thread in threads:
            thread.join()

    def find_similar_words_thread(self, token, threshold, model, flag=None):
      if flag is None:
        try:
            similar_words = model.most_similar(positive=token, topn=2)
            for token, similarity in similar_words:
                if similarity > threshold:
                  if self.tokens_tf_title.get(token) is None:
                    self.tokens_tf_title[token] = 0
                  self.tokens_tf_title[token] += similarity
                  if self.tokens_tf_body.get(token) is None:
                    self.tokens_tf_body[token] = 0
                  self.tokens_tf_body[token] += similarity
        except:
            pass
      elif flag == "title":
        # Function to be executed by each thread
        try:
            similar_words = model.most_similar(positive=token, topn=2)
            for token, similarity in similar_words:
                if similarity > threshold:
                  if self.tokens_tf_title.get(token) is None:
                    self.tokens_tf_title[token] = 0
                  self.tokens_tf_title[token] += similarity
        except:
            pass
      elif flag == 'body':
        # Function to be executed by each thread
        try:
            similar_words = model.most_similar(positive=token, topn=2)
            for token, similarity in similar_words:
                if similarity > threshold:
                  if self.tokens_tf_body.get(token) is None:
                    self.tokens_tf_body[token] = 0
                  self.tokens_tf_body[token] += similarity
        except:
            pass

    def read_posting_list(self, base_dir, w, bucket_name, index):
      TUPLE_SIZE = 6
      TF_MASK = 2 ** 16 - 1 # Masking the 16 low bits of an integer
      with closing(MultiFileReader(base_dir, bucket_name)) as reader:
          locs = index.posting_locs[w]
          b = reader.read(locs, index.df[w][0] * TUPLE_SIZE)
          posting_list = []
          for i in range(index.df[w][0]):
              doc_id = int.from_bytes(b[i*TUPLE_SIZE:i*TUPLE_SIZE+4], 'big')
              tf = int.from_bytes(b[i*TUPLE_SIZE+4:(i+1)*TUPLE_SIZE], 'big')
              posting_list.append((doc_id, tf))
      return posting_list

    def rank_documents(self, pagerank, pageview, docid_title):
      merged_dict = self.bm25_scores.copy()
      heap = []
      res = []
      for doc_id, score in merged_dict.items():
        score = (score / self.max_bm) * 0.5
        if pagerank.get(doc_id) is not None:
            score += (pagerank.get(doc_id) / self.max_pr) * 0.25
        if pageview.get(doc_id) is not None:
            score += (pageview.get(doc_id) / self.max_pv) * 0.25
        merged_dict[doc_id] = score
        heapq.heappush(heap, (-1 * score, str(doc_id)))
      counter = 0
      l = len(heap)
      while counter < self.num and counter < l:
        score, doc_id = heapq.heappop(heap)
        l -= 1
        tup = tuple([doc_id, docid_title.get(int(doc_id))])
        res.append(tup)
        counter += 1
      return res

    # Search section #
    def search_helper(self, query, pagerank, pageview, docid_title, index_title, index_body):
      result = self.process_query(query, pagerank, pageview, docid_title, index_title, index_body)
      return result

